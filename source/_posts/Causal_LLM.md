---
title: 论文分享（二）：On the Reliability of Large Language Models for Causal Discovery
author: 郭世豪
date: 2025-08-22 
categories:  
  - 论文阅读  
tags:  # 标签（多个标签用数组表示）
  - Causal Discovery
  - LLMs' Reliability

---

该研究旨在探究大型语言模型（LLMs）在因果发现中的有效性，通过设计三个实验分别探究记忆、错误信息和上文文信息对因果推断结果的影响。
<!-- more -->



## 一、论文总览

Source: ACL ‘ 25

Code: https://github.com/WilliamsToTo/causality_llm

Abstract: 该研究旨在探究大型语言模型（LLMs）在因果发现中的有效性，通过使用开源 LLMs（OLMO 和 BLOOM，其预训练语料 Dolma 和 ROOTS 可公开访问），围绕三个研究问题展开调查：一是记忆对准确因果关系预测的影响，二是预训练数据中错误因果关系的影响，三是影响 LLMs 理解因果关系的上下文细微差别。研究发现，LLMs 能有效识别预训练数据中频繁出现的因果关系，但泛化到新的或罕见因果关系的能力有限；预训练数据中错误因果关系的存在会显著削弱 LLMs 对相应正确因果关系的信心；此外，上下文信息对 LLMs 辨别随机变量间因果联系的结果至关重要。



## 二、研究背景与目标

因果关系识别是人类认知和科学研究的基础，但传统统计方法存在依赖大量数据、难以精确预测等局限。随着 LLMs 的兴起，其在因果发现中的应用潜力受关注，但存在 “因果鹦鹉” 争议（即 LLMs 可能仅记忆训练数据中的因果关系，而非真正推理）。本研究旨在通过开源 LLMs（OLMO、BLOOM）及其可访问的预训练语料，探究 LLMs 在因果发现中的可靠性，回答三个核心问题。

- RQ 1: 在什么情况下，LLMs 能够可靠且一致地在因果发现中做出准确预测？

- RQ 2: 错误因果关系的存在如何影响 LLMs 在因果发现中的表现？

- RQ 3: 因果关系的上下文信息如何影响 LLMs 在因果发现中的表现？



## 三、方法论

实验研究LLMs在因果推断任务中的可靠性，三步走：

1. 检验与训练数据中支持准确预测的证据
2. 识别预测错误的潜在来源
3. 分析上下文对预测结果的影响

| 真实数据                                                     | 合成数据                                                     |
| ------------------------------------------------------------ | ------------------------------------------------------------ |
| 使用**因果关系模板**（包含指示因果关系的关键词，如”导致“等）收集给定因果关系的提及 | 使用预训练中不存在的变量填入因果关系模板，然后插入随机生成的文档中 |

### 3.1 RQ1：因果推断的依据

- 核心目标

  1. 验证**“因果鹦鹉”假设**（LLMs 预测正确的因果关系，仅仅是因为这些关系在训练数据中被明确提及）
  2. 探究LLMs准确预测的情景

- 评估

  **提及发生与LLMs预测准确性之间的相关性**

  1. 将发生范围分为大致等数量的区间

  2. **评估**（因果问题->是非问题）LLMs在预定义区间的**表现**（准确率和F1-score）

     补充：F1-score计算
     $$
     Precision =\frac{TP} {TP+FP}
     $$

     $$
     Recall= \frac{TP}{TP+FN}
     $$

     $$
     F1- score =2\times\frac{Precision\times Recall}{Precision+Recall}
     $$

     

### 3.2 RQ2：错误因果关系的影响

- 核心目标

  1. 验证”不正确的预测主要源于训练数据中存在语义上相对或否定的因果关系“的推测

- 评估

  **置信度**：在一个查询的生成响应样本中，确认正确因果关系的响应比例

### 3.3 RQ3：上下文信息的影响

本研究产生定量结果，以表明上下文信息的重要性，可能作为另一种预测错误的来源。

- 动机：在不同上下文中LLMs对因果推断的结果不同，与现实世界相吻合

- 评估方法

  1. 对于来自人类注释数据集的每个给定因果关系，使用GPT-4o生成五个肯定该关系的正向上下文和五个否定该关系的负向上下文

  2. 两个LLMs被指示在这些上下文中提供相应的“是/否”问题的答案



## 四、实验设置

### 4.1 数据集

- 任务：**完全因果推断**（一组随机变量）和**因果方向识别**（一组因果相关变量）

- 真实数据：

  | 任务         | 数据集来源                       |
  | ------------ | -------------------------------- |
  | 完全因果推断 | 医学文献、大气科学、汽车保险风险 |
  | 因果方向识别 | Concept Net、Cause Net           |

- 合成数据：

  1. 实验目标：因果方向识别

  2. 构建逻辑：剥离现实语义，控制变量干扰

     随机抽取预训练语料Dolma中100000份文档 → 虚构因果关系 → 正确与错误关系模板化

  3. 三种插入策略

     | 插入策略                                     | 操作逻辑                                                     | 核心目的                                                     |
     | -------------------------------------------- | ------------------------------------------------------------ | ------------------------------------------------------------ |
     | **Correct Relation Scaling（正确关系缩放）** | 对每个正确因果关系（如 “blaonge causes goloneke”），插入次数从 0 到 1000 次梯度变化。 | 测试 “正确因果关系的出现频率” 与 LLMs 判断准确性的相关性：频率越高，模型是否越容易记住并正确识别？（验证 RQ1 的记忆机制） |
     | **Reverse Relation Scaling（反转关系缩放）** | 先固定插入 1000 次正确关系，再对其反转关系（如 “golongeke causes blaonge”）插入 0 到 1000 次。 | 测试 “反转错误关系的频率” 对 LLMs 信心的影响：错误反转越多，模型对正确关系的肯定率是否越低？（验证 RQ2 中反转错误的影响） |
     | **Negated Relation Scaling（否定关系缩放）** | 先固定插入 1000 次正确关系，再对其否定关系（如 “blaonge does not cause goloneke”）插入 0 到 1000 次。 | 测试 “否定错误关系的频率” 对 LLMs 信心的影响：否定表述越多，模型是否越怀疑正确关系的真实性？（验证 RQ2 中否定错误的影响） |

  4. 模型微调：用LoRA高效适配合成数据

### 4.2 模型（上下文学习和提示词工程）

| 模型名称           | 开源状态                               | 预训练语料                      | 对应搜索工具                        |
| ------------------ | -------------------------------------- | ------------------------------- | ----------------------------------- |
| OLMo-7b-Instruct   | 提供预训练语料和模型权重               | Dolma（Soldaini et al., 2024）  | WIMBD（Elazar et al., 2024）        |
| BLOOM-7b1          | 提供预训练语料和模型权重               | ROOTS（Laurençon et al., 2022） | ROOTS Search（Piktus et al., 2023） |
| Llama2-7b-chat     | 仅发布模型权重，不提供预训练语料       | 未提及                          | 无                                  |
| Llama3-8b-Instruct | 仅发布模型权重，不提供预训练语料       | 未提及                          | 无                                  |
| GPT-3.5-turbo      | 闭源模型（不提供预训练语料和模型权重） | 未提及                          | 无                                  |
| GPT-4o             | 闭源模型（不提供预训练语料和模型权重） | 未提及                          | 无                                  |

### 4.3 检索查询

| 模型名称 | 预训练语料 | 对应搜索工具 | 搜索策略                                                     | 搜索限制与补充措施                                           |
| -------- | ---------- | ------------ | ------------------------------------------------------------ | ------------------------------------------------------------ |
| OLMo     | Dolma      | WIMBD        | 1. 精确匹配：检索 “event A causes event B” 完整短语；<br /> 2. 有序短语搜索：检索 “event A⇒causes→event B”，其中 “X⇒Y” 表示 X 在 Y 前且二者位于 32 词文本窗口内。 | 无明确限制，两种策略兼顾精准性与灵活性。                     |
| BLOOM    | ROOTS      | ROOTS Search | 仅支持精确匹配。                                             | 限制：搜索能力有限，无法实现有序短语搜索； <br />补充：通过 **WordNet** 识别同义词，提升检索全面性。 |



## 五、实验结果

### RQ1：LLMs 可靠预测因果关系的条件 —— 依赖记忆高频关系，泛化能力有限

- 关键结果

  ![image-20250824193545277](/images/Causal_LLM/image-20250824193545277.png)

  

  ![image-20250824193700445](/images/Causal_LLM/image-20250824193700445.png)

  ![image-20250824195212836](/images/Causal_LLM/image-20250824195212836.png)

  1. 真实数据验证频率与准确性的强正相关
  2. 合成数据验证记忆机制

- 核心结论

  LLMs 的准确预测高度依赖对预训练数据中**高频因果关系的记忆**，但对低频或全新因果关系（未在训练数据中出现）的泛化能力极弱。这表明 LLMs 更擅长 “复现” 已知因果关系，而非 “发现” 新关系。

### RQ2：错误因果关系的影响——降低对正确关系的信心

- 关键结果

  ![image-20250824195258075](/images/Causal_LLM/image-20250824195258075.png)

  ![image-20250824195352517](/images/Causal_LLM/image-20250824195352517.png)

  1. 真实数据验证错误出现比例与正确关系置信度的负相关
  2. 合成数据验证干扰机制

- 核心结论

  预训练数据中错误因果关系的存在会**显著降低 LLMs 对正确关系的信心**，且错误频率越高，干扰越强。这表明 LLMs 难以辨别冲突信息，训练数据的质量（减少错误因果关系）对性能至关重要。

### RQ3：上下文对因果判断的影响——正面上下文提升性能，负面上下文削弱性能

- 关键结果

  ![image-20250824195852504](/images/Causal_LLM/image-20250824195852504.png)

  上下文对肯定率的显著影响

- 核心结论

  LLMs 对因果关系的判断具有**强上下文敏感性**：正面上下文可增强对正确关系的识别，而负面 / 误导性上下文会严重降低性能。这表明因果发现需纳入具体场景信息，避免脱离上下文的判断。



## 六、总结

- 结论：模型预训练数据中**因果关系的频率**与LLM的表现之间存在正相关关系，而**不正确因果关系的存在**可能会对模型在正确因果关系上的信心产生负面影响。此外，**因果关系的上下文**显著影响因果关系的有效性。

- 创新点分析：
  1. 首次基于开源 LLMs 及其预训练语料，通过实证方法量化验证了 “因果鹦鹉” 假设
  2. 系统性揭示错误因果关系和上下文对LLMs的影响机制
  3. 合成数据控制实验和多维度性能验证
  4. 指出LLMs的局限性（依赖记忆、泛化能力差、易受干扰），为LLM-based因果发现提供优化方向

